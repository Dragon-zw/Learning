`<u><font style="color:#DF2A3F;">kube-scheduler</font></u>`<u> 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。</u>如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。

## 1 调度流程
默认情况下，`<font style="color:#DF2A3F;">kube-scheduler</font>` 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。

`<u><font style="color:#DF2A3F;">kube-scheduler</font></u>`<u><font style="color:#DF2A3F;"> </font></u><u>的主要作用就是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上去，是一个独立的二进制程序，启动之后会一直监听 API Server，获取到 </u>`<u><font style="color:#DF2A3F;">PodSpec.NodeName</font></u>`<u> 为空的 Pod，对每个 Pod 都会创建一个 binding。</u>

![](https://cdn.nlark.com/yuque/0/2024/png/2555283/1731570818807-9a571eb6-8b24-46af-989a-2a655b561f01.png)

这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：

+ 如何保证全部的节点调度的公平性？要知道并不是所有节点资源配置一定都是一样的
+ 如何保证每个节点都能被分配资源？
+ 集群资源如何能够被高效利用？
+ 集群资源如何才能被最大化使用？
+ 如何保证 Pod 调度的性能和效率？
+ 用户是否可以根据自己的实际需求定制自己的调度策略？

考虑到实际环境中的各种复杂情况，<u>kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</u>

kubernetes 调度器的源码位于 `<font style="color:#DF2A3F;">kubernetes/pkg/scheduler</font>` 中，其中 Scheduler 创建和运行的核心程序，对应的代码在 `<font style="color:#DF2A3F;">pkg/scheduler/scheduler.go</font>`，如果要查看 `<font style="color:#DF2A3F;">kube-scheduler</font>` 的入口程序，对应的代码在 `<font style="color:#DF2A3F;">cmd/kube-scheduler/scheduler.go</font>`。

调度主要分为以下几个部分：

+ <u>首先是</u>**<u><font style="color:#DF2A3F;">预选过程</font></u>**<u>，过滤掉不满足条件的节点，这个过程称为 </u>`<u><font style="color:#DF2A3F;">Predicates</font></u>`<u>（过滤）</u>
+ <u>然后是</u>**<u><font style="color:#DF2A3F;">优选过程</font></u>**<u>，对通过的节点按照优先级排序，称之为 </u>`<u><font style="color:#DF2A3F;">Priorities</font></u>`<u>（打分）</u>
+ 最后从中选择优先级最高的节点，如果中间任何一步骤有错误，就直接返回错误

`<font style="color:#DF2A3F;">Predicates</font>` 阶段首先遍历全部节点，过滤掉不满足条件的节点，属于`<font style="color:#DF2A3F;">强制性</font>`规则，这一阶段输出的所有满足要求的节点将被记录并作为第二阶段的输入，如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试。

所以我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。

`<font style="color:#DF2A3F;">Priorities</font>` 阶段即再次对节点进行筛选，如果有多个节点都满足条件的话，那么系统会按照节点的优先级(`<font style="color:#DF2A3F;">priorites</font>`)大小对节点进行排序，最后选择优先级最高的节点来部署 Pod 应用。

下面是调度过程的简单示意图：

![](https://cdn.nlark.com/yuque/0/2024/png/2555283/1731570817170-05667a27-1dc5-4745-bb9a-a83f6d612017.png)

更详细的流程是这样的：

+ 首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源
+ API Server 收到用户请求后，存储相关数据到 etcd 数据库中
+ 调度器监听 API Server 查看到还未被调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点，这个分配过程就是我们上面提到的两个阶段：
    - 预选阶段(Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉
    - 优选阶段(Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本尽量分布到不同的主机上，使用最低负载的主机等等策略
+ 经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 `<font style="color:#DF2A3F;">binding</font>` 操作，然后将结果存储到 etcd 中 最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作（当然也是 watch APIServer 发现的）。

目前调度器已经全部通过插件的方式实现了调度框架，默认开启的调度插件如以下代码所示：

```go
// pkg/scheduler/algorithmprovider/registry.go

func getDefaultConfig() *schedulerapi.Plugins {
    return &schedulerapi.Plugins{
        QueueSort: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: queuesort.Name},
            },
        },
        PreFilter: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: noderesources.FitName},
                {Name: nodeports.Name},
                {Name: podtopologyspread.Name},
                {Name: interpodaffinity.Name},
                {Name: volumebinding.Name},
            },
        },
        Filter: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: nodeunschedulable.Name},
                {Name: noderesources.FitName},
                {Name: nodename.Name},
                {Name: nodeports.Name},
                {Name: nodeaffinity.Name},
                {Name: volumerestrictions.Name},
                {Name: tainttoleration.Name},
                {Name: nodevolumelimits.EBSName},
                {Name: nodevolumelimits.GCEPDName},
                {Name: nodevolumelimits.CSIName},
                {Name: nodevolumelimits.AzureDiskName},
                {Name: volumebinding.Name},
                {Name: volumezone.Name},
                {Name: podtopologyspread.Name},
                {Name: interpodaffinity.Name},
            },
        },
        PostFilter: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: defaultpreemption.Name},
            },
        },
        PreScore: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: interpodaffinity.Name},
                {Name: podtopologyspread.Name},
                {Name: tainttoleration.Name},
            },
        },
        Score: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: noderesources.BalancedAllocationName, Weight: 1},
                {Name: imagelocality.Name, Weight: 1},
                {Name: interpodaffinity.Name, Weight: 1},
                {Name: noderesources.LeastAllocatedName, Weight: 1},
                {Name: nodeaffinity.Name, Weight: 1},
                {Name: nodepreferavoidpods.Name, Weight: 10000},
                // Weight is doubled because:
                // - This is a score coming from user preference.
                // - It makes its signal comparable to NodeResourcesLeastAllocated.
                {Name: podtopologyspread.Name, Weight: 2},
                {Name: tainttoleration.Name, Weight: 1},
            },
        },
        Reserve: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: volumebinding.Name},
            },
        },
        PreBind: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: volumebinding.Name},
            },
        },
        Bind: &schedulerapi.PluginSet{
            Enabled: []schedulerapi.Plugin{
                {Name: defaultbinder.Name},
            },
        },
    }
}
```

从上面我们可以看出调度器的一系列算法由各种插件在调度的不同阶段来完成，下面我们就先来了解下调度框架。

### 1.1 调度框架
调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑（我们称之为**<font style="color:#DF2A3F;">扩展</font>**），并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知。

我们知道每当调度一个 Pod 时，都会按照两个过程来执行：调度过程和绑定过程。

调度过程为 Pod 选择一个合适的节点，绑定过程则将调度过程的决策应用到集群中（也就是在被选定的节点上运行 Pod），将调度过程和绑定过程合在一起，称之为**<font style="color:#DF2A3F;">调度上下文（scheduling context）</font>**。需要注意的是调度过程是`<font style="color:#DF2A3F;">同步</font>`运行的（同一时间点只为一个 Pod 进行调度），绑定过程可异步运行（同一时间点可并发为多个 Pod 执行绑定）。

调度过程和绑定过程遇到如下情况时会中途退出：

+ 调度程序认为当前没有该 Pod 的可选节点
+ 内部错误

这个时候，该 Pod 将被放回到 **<font style="color:#DF2A3F;">待调度队列</font>**，并等待下次重试。

#### 1.1.1 扩展点（Extension Points）
下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。

![](https://cdn.nlark.com/yuque/0/2024/png/2555283/1731570816571-ca396c3e-ed89-4bd3-89ba-5527fd06378e.png)

1. `<font style="color:#DF2A3F;">QueueSort</font>` 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，`<font style="color:#DF2A3F;">QueueSort</font>` 扩展本质上只需要实现一个方法 `<font style="color:#DF2A3F;">Less(Pod1, Pod2)</font>` 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 `<font style="color:#DF2A3F;">QueueSort</font>` 插件生效。
2. `<font style="color:#DF2A3F;">Pre-filter</font>` 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 `<font style="color:#DF2A3F;">pre-filter</font>` 返回了 error，则调度过程终止。
3. `<font style="color:#DF2A3F;">Filter</font>` 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 `<font style="color:#DF2A3F;">filter</font>` 扩展；如果任何一个 `<font style="color:#DF2A3F;">filter</font>` 将节点标记为不可选，则余下的 `<font style="color:#DF2A3F;">filter</font>` 扩展将不会被执行。调度器可以同时对多个节点执行 `<font style="color:#DF2A3F;">filter</font>` 扩展。
4. `<font style="color:#DF2A3F;">Post-filter</font>` 是一个通知类型的扩展点，调用该扩展的参数是 `<font style="color:#DF2A3F;">filter</font>` 阶段结束后被筛选为**<font style="color:#DF2A3F;">可选节点</font>**的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。
5. `<font style="color:#DF2A3F;">Scoring</font>` 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 `<font style="color:#DF2A3F;">Soring</font>` 扩展，评分结果是一个范围内的整数。在 `<font style="color:#DF2A3F;">normalize scoring</font>` 阶段，调度器将会把每个 `<font style="color:#DF2A3F;">scoring</font>` 扩展对具体某个节点的评分结果和该扩展的权重合并起来，作为最终评分结果。
6. `<font style="color:#DF2A3F;">Normalize scoring</font>` 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 `<font style="color:#DF2A3F;">scoring</font>` 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 `<font style="color:#DF2A3F;">normalize scoring</font>`<font style="color:#DF2A3F;"> </font>扩展一次。
7. `<font style="color:#DF2A3F;">Reserve</font>` 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 reserved 状态以后，要么在绑定失败时触发 Unreserve 扩展，要么在绑定成功时，由 Post-bind 扩展结束绑定过程。
8. `<font style="color:#DF2A3F;">Permit</font>` 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项：
    - `<font style="color:#DF2A3F;">approve</font>`（批准）：当所有的 permit 扩展都 approve 了 Pod 与节点的绑定，调度器将继续执行绑定过程
    - `<font style="color:#DF2A3F;">deny</font>`（拒绝）：如果任何一个 permit 扩展 deny 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 `<font style="color:#DF2A3F;">Unreserve</font>` 扩展
    - `<font style="color:#DF2A3F;">wait</font>`（等待）：如果一个 permit 扩展返回了 wait，则 Pod 将保持在 permit 阶段，直到被其他扩展 approve，如果超时事件发生，wait 状态变成 deny，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展
9. `<font style="color:#DF2A3F;">Pre-bind</font>` 扩展用于在 Pod 绑定之前执行某些逻辑。例如，pre-bind 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 `<font style="color:#DF2A3F;">pre-bind</font>` 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展。
10. `<font style="color:#DF2A3F;">Bind</font>` 扩展用于将 Pod 绑定到节点上：
    - 只有所有的 pre-bind 扩展都成功执行了，bind 扩展才会执行
    - 调度框架按照 bind 扩展注册的顺序逐个调用 bind 扩展
    - 具体某个 bind 扩展可以选择处理或者不处理该 Pod
    - 如果某个 bind 扩展处理了该 Pod 与节点的绑定，余下的 bind 扩展将被忽略
11. `<font style="color:#DF2A3F;">Post-bind</font>` 是一个通知性质的扩展：
    - `<font style="color:#DF2A3F;">Post-bind</font>` 扩展在 Pod 成功绑定到节点上之后被动调用
    - `<font style="color:#DF2A3F;">Post-bind</font>` 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作
12. `<font style="color:#DF2A3F;">Unreserve</font>` 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 unreserve 扩展将被调用。Unreserve 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，reserve 扩展和 unreserve 扩展应该成对出现。

如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现扩展点接口，对应的扩展点接口我们可以在源码 `<font style="color:#DF2A3F;">pkg/scheduler/framework/v1alpha1/interface.go</font>` 文件中找到，如下所示：

```go
// Plugin is the parent type for all the scheduling framework plugins.
type Plugin interface {
    Name() string
}

type QueueSortPlugin interface {
    Plugin
    Less(*PodInfo, *PodInfo) bool
}

// PreFilterPlugin is an interface that must be implemented by "prefilter" plugins.
// These plugins are called at the beginning of the scheduling cycle.
type PreFilterPlugin interface {
    Plugin
    PreFilter(pc *PluginContext, p *v1.Pod) *Status
}

// FilterPlugin is an interface for Filter plugins. These plugins are called at the
// filter extension point for filtering out hosts that cannot run a pod.
// This concept used to be called 'predicate' in the original scheduler.
// These plugins should return "Success", "Unschedulable" or "Error" in Status.code.
// However, the scheduler accepts other valid codes as well.
// Anything other than "Success" will lead to exclusion of the given host from
// running the pod.
type FilterPlugin interface {
    Plugin
    Filter(pc *PluginContext, pod *v1.Pod, nodeName string) *Status
}

// PostFilterPlugin is an interface for Post-filter plugin. Post-filter is an
// informational extension point. Plugins will be called with a list of nodes
// that passed the filtering phase. A plugin may use this data to update internal
// state or to generate logs/metrics.
type PostFilterPlugin interface {
    Plugin
    PostFilter(pc *PluginContext, pod *v1.Pod, nodes []*v1.Node, filteredNodesStatuses NodeToStatusMap) *Status
}

// ScorePlugin is an interface that must be implemented by "score" plugins to rank
// nodes that passed the filtering phase.
type ScorePlugin interface {
    Plugin
    Score(pc *PluginContext, p *v1.Pod, nodeName string) (int, *Status)
}

// ScoreWithNormalizePlugin is an interface that must be implemented by "score"
// plugins that also need to normalize the node scoring results produced by the same
// plugin's "Score" method.
type ScoreWithNormalizePlugin interface {
    ScorePlugin
    NormalizeScore(pc *PluginContext, p *v1.Pod, scores NodeScoreList) *Status
}

// ReservePlugin is an interface for Reserve plugins. These plugins are called
// at the reservation point. These are meant to update the state of the plugin.
// This concept used to be called 'assume' in the original scheduler.
// These plugins should return only Success or Error in Status.code. However,
// the scheduler accepts other valid codes as well. Anything other than Success
// will lead to rejection of the pod.
type ReservePlugin interface {
    Plugin
    Reserve(pc *PluginContext, p *v1.Pod, nodeName string) *Status
}

// PreBindPlugin is an interface that must be implemented by "prebind" plugins.
// These plugins are called before a pod being scheduled.
type PreBindPlugin interface {
    Plugin
    PreBind(pc *PluginContext, p *v1.Pod, nodeName string) *Status
}

// PostBindPlugin is an interface that must be implemented by "postbind" plugins.
// These plugins are called after a pod is successfully bound to a node.
type PostBindPlugin interface {
    Plugin
    PostBind(pc *PluginContext, p *v1.Pod, nodeName string)
}

// UnreservePlugin is an interface for Unreserve plugins. This is an informational
// extension point. If a pod was reserved and then rejected in a later phase, then
// un-reserve plugins will be notified. Un-reserve plugins should clean up state
// associated with the reserved Pod.
type UnreservePlugin interface {
    Plugin
    Unreserve(pc *PluginContext, p *v1.Pod, nodeName string)
}

// PermitPlugin is an interface that must be implemented by "permit" plugins.
// These plugins are called before a pod is bound to a node.
type PermitPlugin interface {
    Plugin
    Permit(pc *PluginContext, p *v1.Pod, nodeName string) (*Status, time.Duration)
}

// BindPlugin is an interface that must be implemented by "bind" plugins. Bind
// plugins are used to bind a pod to a Node.
type BindPlugin interface {
    Plugin
    Bind(pc *PluginContext, p *v1.Pod, nodeName string) *Status
}
```

对于调度框架插件的启用或者禁用，我们可以使用安装集群时的 [KubeSchedulerConfiguration](https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration) 资源对象来进行配置。下面的例子中的配置启用了一个实现了 `<font style="color:#DF2A3F;">reserve</font>` 和 `<font style="color:#DF2A3F;">preBind</font>` 扩展点的插件，并且禁用了另外一个插件，同时为插件 foo 提供了一些配置信息：

```yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration

---
plugins:
  reserve:
    enabled:
      - name: foo
      - name: bar
    disabled:
      - name: baz
  preBind:
    enabled:
      - name: foo
    disabled:
      - name: baz

pluginConfig:
  - name: foo
    args: >
      foo插件可以解析的任意内容
```

扩展的调用顺序如下：

+ 如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展
+ 如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展
+ 默认插件的扩展始终被最先调用，然后按照 `<font style="color:#DF2A3F;">KubeSchedulerConfiguration</font>` 中扩展的激活 `<font style="color:#DF2A3F;">enabled</font>`<font style="color:#DF2A3F;"> </font>顺序逐个调用扩展点的扩展
+ 可以先禁用默认插件的扩展，然后在 `<font style="color:#DF2A3F;">enabled</font>` 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序

假设默认插件 foo 实现了 `<font style="color:#DF2A3F;">reserve</font>` 扩展点，此时我们要添加一个插件 bar，想要在 foo 之前被调用，则应该先禁用 foo 再按照 bar foo 的顺序激活。示例配置如下所示：

```yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration

---
plugins:
  reserve:
    enabled:
      - name: bar
      - name: foo
    disabled:
      - name: foo
```

在源码目录 `<font style="color:#DF2A3F;">pkg/scheduler/framework/plugins/examples</font>` 中有几个示范插件，我们可以参照其实现方式。

#### 1.1.2 示例
:::color2
需要使用对应较低版本的 Kubernetes 的版本，大概在 V1.22 版本左右，否则会无法正常使用对应的第三方的 `<font style="color:#DF2A3F;">scheduler</font>`<font style="color:#DF2A3F;"> </font>的插件。

:::

其实要实现一个调度框架的插件，并不难，我们只要实现对应的扩展点，然后将插件注册到调度器中即可，下面是默认调度器在初始化的时候注册的插件：

```go
// pkg/scheduler/algorithmprovider/registry.go
func NewRegistry() Registry {
    return Registry{
        // FactoryMap:
        // New plugins are registered here.
        // example:
        // {
        //  stateful_plugin.Name: stateful.NewStatefulMultipointExample,
        //  fooplugin.Name: fooplugin.New,
        // }
    }
}
```

但是可以看到默认并没有注册一些插件，所以要想让调度器能够识别我们的插件代码，就需要自己来实现一个调度器了，当然这个调度器我们完全没必要完全自己实现，直接调用默认的调度器，然后在上面的 `<font style="color:#DF2A3F;">NewRegistry()</font>` 函数中将我们的插件注册进去即可。在 `<font style="color:#DF2A3F;">kube-scheduler</font>` 的源码文件 `<font style="color:#DF2A3F;">kubernetes/cmd/kube-scheduler/app/server.go</font>` 中有一个 `<font style="color:#DF2A3F;">NewSchedulerCommand</font>` 入口函数，其中的参数是一个类型为 `<font style="color:#DF2A3F;">Option</font>` 的列表，而这个 `<font style="color:#DF2A3F;">Option</font>` 恰好就是一个插件配置的定义：

```go
// Option configures a framework.Registry.
type Option func(framework.Registry) error

// NewSchedulerCommand creates a *cobra.Command object with default parameters and registryOptions
func NewSchedulerCommand(registryOptions ...Option) *cobra.Command {
  ......
}
```

所以我们完全就可以直接调用这个函数来作为我们的函数入口，并且传入我们自己实现的插件作为参数即可，而且该文件下面还有一个名为 `<font style="color:#DF2A3F;">WithPlugin</font>` 的函数可以来创建一个 `<font style="color:#DF2A3F;">Option</font>` 实例：

```go
// WithPlugin creates an Option based on plugin name and factory.
func WithPlugin(name string, factory framework.PluginFactory) Option {
    return func(registry framework.Registry) error {
        return registry.Register(name, factory)
    }
}
```

所以最终我们的入口函数如下所示：

```go
func main() {
    rand.Seed(time.Now().UTC().UnixNano())

    command := app.NewSchedulerCommand(
        app.WithPlugin(sample.Name, sample.New),
    )

    logs.InitLogs()
    defer logs.FlushLogs()

    if err := command.Execute(); err != nil {
        _, _ = fmt.Fprintf(os.Stderr, "%v\n", err)
        os.Exit(1)
    }

}
```

其中 `<font style="color:#DF2A3F;">app.WithPlugin(sample.Name, sample.New)</font>` 就是我们接下来要实现的插件，从 `<font style="color:#DF2A3F;">WithPlugin</font>` 函数的参数也可以看出我们这里的 `<font style="color:#DF2A3F;">sample.New</font>` 必须是一个 `<font style="color:#DF2A3F;">framework.PluginFactory</font>` 类型的值，而 `<font style="color:#DF2A3F;">PluginFactory</font>` 的定义就是一个函数：

```go
type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)
```

所以 `<font style="color:#DF2A3F;">sample.New</font>` 实际上就是上面的这个函数，在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可，插件实现如下所示，我们这里只是简单获取下数据打印日志，如果你有实际需求的可以根据获取的数据就行处理即可，我们这里只是实现了 `<font style="color:#DF2A3F;">PreFilter</font>`、`<font style="color:#DF2A3F;">Filter</font>`、`<font style="color:#DF2A3F;">PreBind</font>` 三个扩展点，其他的可以用同样的方式来扩展即可：

```go
// 插件名称
const Name = "sample-plugin"

type Args struct {
    FavoriteColor  string `json:"favorite_color,omitempty"`
    FavoriteNumber int    `json:"favorite_number,omitempty"`
    ThanksTo       string `json:"thanks_to,omitempty"`
}

type Sample struct {
    args   *Args
    handle framework.FrameworkHandle
}

func (s *Sample) Name() string {
    return Name
}

func (s *Sample) PreFilter(pc *framework.PluginContext, pod *v1.Pod) *framework.Status {
    klog.V(3).Infof("prefilter pod: %v", pod.Name)
    return framework.NewStatus(framework.Success, "")
}

func (s *Sample) Filter(pc *framework.PluginContext, pod *v1.Pod, nodeName string) *framework.Status {
    klog.V(3).Infof("filter pod: %v, node: %v", pod.Name, nodeName)
    return framework.NewStatus(framework.Success, "")
}

func (s *Sample) PreBind(pc *framework.PluginContext, pod *v1.Pod, nodeName string) *framework.Status {
    if nodeInfo, ok := s.handle.NodeInfoSnapshot().NodeInfoMap[nodeName]; !ok {
        return framework.NewStatus(framework.Error, fmt.Sprintf("prebind get node info error: %+v", nodeName))
    } else {
        klog.V(3).Infof("prebind node info: %+v", nodeInfo.Node())
        return framework.NewStatus(framework.Success, "")
    }
}

//type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)
func New(configuration *runtime.Unknown, f framework.FrameworkHandle) (framework.Plugin, error) {
    args := &Args{}
    if err := framework.DecodeInto(configuration, args); err != nil {
        return nil, err
    }
    klog.V(3).Infof("get plugin config args: %+v", args)
    return &Sample{
        args: args,
        handle: f,
    }, nil
}
```

完整代码可以前往仓库 [https://github.com/cnych/sample-scheduler-framework](https://github.com/cnych/sample-scheduler-framework) 获取。

实现完成后，编译打包成镜像即可，然后我们就可以当成普通的应用用一个 `<font style="color:#DF2A3F;">Deployment</font>` 控制器来部署即可，由于我们需要去获取集群中的一些资源对象，所以当然需要申请 RBAC 权限，然后同样通过 `<font style="color:#DF2A3F;">--config</font>` 参数来配置我们的调度器，同样还是使用一个 `<font style="color:#DF2A3F;">KubeSchedulerConfiguration</font>` 资源对象配置，可以通过 `<font style="color:#DF2A3F;">plugins</font>` 来启用或者禁用我们实现的插件，也可以通过 `<font style="color:#DF2A3F;">pluginConfig</font>` 来传递一些参数值给插件：

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: sample-scheduler-clusterrole
rules:
  - apiGroups:
      - ''
    resources:
      - endpoints
      - events
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - ''
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - pods
    verbs:
      - delete
      - get
      - list
      - watch
      - update
  - apiGroups:
      - ''
    resources:
      - bindings
      - pods/binding
    verbs:
      - create
  - apiGroups:
      - ''
    resources:
      - pods/status
    verbs:
      - patch
      - update
  - apiGroups:
      - ''
    resources:
      - replicationcontrollers
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
      - extensions
    resources:
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - persistentvolumeclaims
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ''
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - 'storage.k8s.io'
    resources:
      - storageclasses
      - csinodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - 'coordination.k8s.io'
    resources:
      - leases
    verbs:
      - create
      - get
      - list
      - update
  - apiGroups:
      - 'events.k8s.io'
    resources:
      - events
    verbs:
      - create
      - patch
      - update
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sample-scheduler-sa
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: sample-scheduler-clusterrolebinding
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sample-scheduler-clusterrole
subjects:
  - kind: ServiceAccount
    name: sample-scheduler-sa
    namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta1
    kind: KubeSchedulerConfiguration
    leaderElection:
      leaderElect: true
      leaseDuration: 15s
      renewDeadline: 10s
      resourceLock: endpointsleases
      resourceName: sample-scheduler
      resourceNamespace: kube-system
      retryPeriod: 2s
    profiles:
      - schedulerName: sample-scheduler
        plugins:
          preFilter:
            enabled:
              - name: "sample-plugin"
          filter:
            enabled:
              - name: "sample-plugin"
        pluginConfig:
          - name: sample-plugin
            args:  # runtime.Object
              favorColor: "#326CE5"
              favorNumber: 7
              thanksTo: "Kubernetes"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-scheduler
  namespace: kube-system
  labels:
    component: sample-scheduler
spec:
  selector:
    matchLabels:
      component: sample-scheduler
  template:
    metadata:
      labels:
        component: sample-scheduler
    spec:
      serviceAccountName: sample-scheduler-sa
      priorityClassName: system-cluster-critical
      volumes:
        - name: scheduler-config
          configMap:
            name: scheduler-config
      containers:
        - name: scheduler
          image: cnych/sample-scheduler:v0.2.4
          imagePullPolicy: IfNotPresent
          command:
            - sample-scheduler
            - --config=/etc/kubernetes/scheduler-config.yaml
            - --v=3
          volumeMounts:
            - name: scheduler-config
              mountPath: /etc/kubernetes
#          livenessProbe:
#            httpGet:
#              path: /healthz
#              port: 10251
#            initialDelaySeconds: 15
#          readinessProbe:
#            httpGet:
#              path: /healthz
#              port: 10251
```

直接部署上面的资源对象即可，这样我们就部署了一个名为<font style="color:#DF2A3F;"> </font>`<font style="color:#DF2A3F;">sample-scheduler</font>` 的调度器了

```shell
$ kubectl create -f sample-scheduler.yaml 
clusterrole.rbac.authorization.k8s.io/sample-scheduler-clusterrole created
serviceaccount/sample-scheduler-sa created
clusterrolebinding.rbac.authorization.k8s.io/sample-scheduler-clusterrolebinding created
configmap/scheduler-config created
deployment.apps/sample-scheduler created

$ kubectl get deployment -n kube-system sample-scheduler
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
sample-scheduler   1/1     1            1           50s
```

接下来我们可以部署一个应用来使用这个调度器进行调度：

```yaml
# Deployment-test-scheduler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-scheduler
spec:
  selector:
    matchLabels:
      app: test-scheduler
  template:
    metadata:
      labels:
        app: test-scheduler
    spec:
      schedulerName: sample-scheduler # 指定使用的调度器，不指定使用默认的default-scheduler
      containers:
        - image: nginx:1.7.9
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
            - containerPort: 80
```

这里需要注意的是我们现在手动指定了一个 `<font style="color:#DF2A3F;">schedulerName</font>` 的字段，将其设置成上面我们自定义的调度器名称 `<font style="color:#DF2A3F;">sample-scheduler</font>`。

我们直接创建这个资源对象，创建完成后查看我们自定义调度器的日志信息：

```shell
➜ kubectl get pods -n kube-system -l component=sample-scheduler
NAME                               READY   STATUS    RESTARTS   AGE
sample-scheduler-896658cd7-k7vcl   1/1     Running   0          55s

➜ kubectl logs -f $(kubectl get pods -n kube-system -l component=sample-scheduler -o name | awk -F'/' '{print $2}') -n kube-system
I0114 09:14:18.878613       1 eventhandlers.go:173] add event for unscheduled pod default/test-scheduler-6486fd49fc-zjhcx
I0114 09:14:18.878670       1 scheduler.go:464] Attempting to schedule pod: default/test-scheduler-6486fd49fc-zjhcx
I0114 09:14:18.878706       1 sample.go:77] "Start PreFilter Pod" pod="test-scheduler-6486fd49fc-zjhcx"
I0114 09:14:18.878802       1 sample.go:93] "Start Filter Pod" pod="test-scheduler-6486fd49fc-zjhcx" node="node2" preFilterState=&{Resource:{MilliCPU:0 Memory:0 EphemeralStorage:0 AllowedPodNumber:0 ScalarResources:map[]}}
I0114 09:14:18.878835       1 sample.go:93] "Start Filter Pod" pod="test-scheduler-6486fd49fc-zjhcx" node="node1" preFilterState=&{Resource:{MilliCPU:0 Memory:0 EphemeralStorage:0 AllowedPodNumber:0 ScalarResources:map[]}}
I0114 09:14:18.879043       1 default_binder.go:51] Attempting to bind default/test-scheduler-6486fd49fc-zjhcx to node1
I0114 09:14:18.886360       1 scheduler.go:609] "Successfully bound pod to node" pod="default/test-scheduler-6486fd49fc-zjhcx" node="node1" evaluatedNodes=3 feasibleNodes=2
I0114 09:14:18.887426       1 eventhandlers.go:205] delete event for unscheduled pod default/test-scheduler-6486fd49fc-zjhcx
I0114 09:14:18.887475       1 eventhandlers.go:225] add event for scheduled pod default/test-scheduler-6486fd49fc-zjhcx
```

可以看到当我们创建完 Pod 后，在我们自定义的调度器中就出现了对应的日志，并且在我们定义的扩展点上面都出现了对应的日志，证明我们的示例成功了，也可以通过查看 Pod 的 `<font style="color:#DF2A3F;">schedulerName</font>` 来验证：

```shell
➜ kubectl get pods
NAME                              READY   STATUS    RESTARTS       AGE
test-scheduler-6486fd49fc-zjhcx   1/1     Running   0              35s

➜ kubectl get pod test-scheduler-6486fd49fc-zjhcx -o yaml
......
restartPolicy: Always
schedulerName: sample-scheduler
securityContext: {}
serviceAccount: default
......
```

从 Kubernetes v1.17 版本开始，`**<u><font style="color:#DF2A3F;">Scheduler Framework</font></u>**`**<u> 内置的预选和优选函数已经全部插件化</u>**，所以要扩展调度器我们应该掌握并理解调度框架这种方式。

## 2 调度器调优
<u><font style="color:#DF2A3F;">作为 kubernetes 集群的默认调度器，kube-scheduler 主要负责将 Pod 调度到集群的 Node 上。</font></u>在一个集群中，满足一个 Pod 调度请求的所有节点称之为<font style="color:#DF2A3F;"> </font>`<font style="color:#DF2A3F;">可调度 Node</font>`，调度器先在集群中找到一个 Pod 的可调度 Node，然后<u><font style="color:#DF2A3F;">根据一系列函数对这些可调度 Node 进行打分</font></u>，之后选出其中得分最高的 Node 来运行 Pod，最后，调度器将这个调度决定告知 `<font style="color:#DF2A3F;">kube-apiserver</font>`，这个过程叫做**<font style="color:#DF2A3F;">绑定</font>**。

在 Kubernetes 1.12 版本之前，kube-scheduler 会检查集群中所有节点的可调度性，并且给可调度节点打分。Kubernetes 1.12 版本添加了一个新的功能，<u><font style="color:#DF2A3F;">允许调度器在找到一定数量的可调度节点之后就停止继续寻找可调度节点。</font></u>该功能能提高调度器在大规模集群下的调度性能，这个数值是集群规模的百分比，这个百分比通过 `<font style="color:#DF2A3F;">percentageOfNodesToScore</font>` 参数来进行配置，其值的范围在 1 到 100 之间，最大值就是 100%，如果设置为 0 就代表没有提供这个参数配置。Kubernetes 1.14 版本又加入了一个特性，在该参数没有被用户配置的情况下，调度器会根据集群的规模自动设置一个集群比例，然后通过这个比例筛选一定数量的可调度节点进入打分阶段。该特性使用**<font style="color:#DF2A3F;">线性公式</font>**计算出集群比例，比如 100 个节点的集群下会取 50%，在 5000 节点的集群下取 10%，这个自动设置的参数的最低值是 5%，换句话说，<u><font style="color:#DF2A3F;">调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。</font></u>

:::color2
⚠️"注意"

当集群中的可调度节点 Node 少于 50 个时，调度器仍然会去检查所有节点，因为可调度节点太少，不足以停止调度器最初的过滤选择。如果我们想要关掉这个范围参数，可以将 `<font style="color:#DF2A3F;">percentageOfNodesToScore</font>` 值设置成 100。

:::

`<font style="color:#DF2A3F;">percentageOfNodesToScore</font>` 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的，另外 **50** 个 Node 的数值是硬编码在程序里面的，设置这个值的作用在于：**<font style="color:#DF2A3F;">当集群的规模是数百个节点并且 percentageOfNodesToScore 参数设置的过低的时候，调度器筛选到的可调度节点数目基本不会受到该参数影响</font>**。当集群规模较小时，这个设置对调度器性能提升并不明显，但是在超过 1000 个 Node 的集群中，将调优参数设置为一个较低的值可以很明显的提升调度器性能。

不过值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，很多 Node 都没有进入到打分阶段，这样就会造成一种后果，一个本来可以在打分阶段得分很高的 Node 甚至都不能进入打分阶段，由于这个原因，所以这个参数不应该被设置成一个很低的值，通常的做法是不会将这个参数的值设置的低于 10，很低的参数值一般在调度器的吞吐量很高且对 Node 的打分不重要的情况下才使用。换句话说，只有当你更倾向于在可调度节点中任意选择一个 Node 来运行这个 Pod 时，才使用很低的参数设置。

:::color2
<u>如果你的集群规模只有数百个节点或者更少，实际上并不推荐你将这个参数设置得比默认值更低，因为这种情况下不太会有效的提高调度器性能。</u>

:::

## 3 优先级调度
与前面所讲的调度优选策略中的优先级（Priorities）不同，前面所讲的优先级指的是节点优先级，而我们这里所说的优先级指的是 Pod 的优先级，高优先级的 Pod 会优先被调度，或者在资源不足的情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。

要定义 Pod 优先级，就需要先定义 `<font style="color:#DF2A3F;">PriorityClass</font>` 对象，该对象没有 Namespace 的限制：

```yaml
# 旧版本的 Kubernetes
# PriorityClass-high-priority.yaml
apiVersion: v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false # 设置为 false
description: 'This priority class should be used for XYZ service pods only.'

---
# 新版本的 Kubernetes 
# PriorityClass-high-priority.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false # 设置为 false
description: 'This priority class should be used for XYZ service pods only.'
```

其中：

+ value 为 32 位整数的优先级，该值越大，优先级越高
+ `<font style="color:#DF2A3F;">globalDefault</font>` 用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个 `<font style="color:#DF2A3F;">PriorityClass</font>` 将其设置为 `<font style="color:#DF2A3F;">true</font>`

```shell
$ kubectl create -f PriorityClass-high-priority.yaml 
priorityclass.scheduling.k8s.io/high-priority created
```

然后通过在 Pod 的 `<font style="color:#DF2A3F;">spec.priorityClassName</font>` 中指定已定义的 `<font style="color:#DF2A3F;">PriorityClass</font>` 名称即可：

```yaml
# Pod-nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
```

引用资源清单文件

```shell
$ kubectl create -f Pod-nginx.yaml 
pod/nginx created

# 查看 Pod 的优先级
$ kubectl get pod nginx -o yaml | grep "priority"
  priority: 1000000
  priorityClassName: high-priority

# 查看其他 Pod 的优先级
$ kubectl get pod static-web-hkk8smaster001 -o yaml | grep "priority"
  priority: 0 # 不定义那么就是 0
```

另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 Pending 时，抢占（preemption）逻辑就会被触发，抢占会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。

